# Advanced-Algorithms--Ensemble-Methods

## What are Ensemble Methods??
Ensemble learning is a machine learning paradigm where multiple models (often called “weak learners”) are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined, we can obtain more accurate and/or robust models. 

Two families of ensemble methods are usually distinguished:

1• In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. Examples: Bagging methods-Random Forest


2• By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble. Examples: Boosting methods-XGBoost.



Here we are discussing following advanced algorithms: 
1. Bagging
2. AdaBoost
3. GBM- Gradient Boosting Machines
4. Light BGM
5. XGBoost (Extreme Gradient)
6. CatBoost
